{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils.general_utils import get_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation. They can then call self.config.<hyperparameter_name> to\n",
    "    get the hyperparameter settings.\n",
    "    \"\"\"\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    batch_size = 64\n",
    "    n_epochs = 50\n",
    "    lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                                              (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                                              (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,self.config.n_features])\n",
    "        self.labels_placeholder = tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size,self.config.n_classes])\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be the placeholder\n",
    "                tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {self.input_placeholder: inputs_batch}\n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
    "        softmax transformation:\n",
    "\n",
    "        yhat = softmax(xW + b)\n",
    "\n",
    "        Hint: The input x will be passed in through self.input_placeholder. Each ROW of\n",
    "              self.input_placeholder is a single example. This is usually best-practice for\n",
    "              tensorflow code.\n",
    "        Hint: Make sure to create tf.Variables as needed.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                    and biases b with zeros.\n",
    "\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        W = tf.Variable(tf.zeros([self.config.n_features,self.config.n_classes]))\n",
    "        b = tf.Variable(tf.zeros([self.config.batch_size,1]))\n",
    "        pred = softmax(tf.matmul(self.input_placeholder,W)+b)\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                    short function.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = cross_entropy_loss(y=self.labels_placeholder,yhat=pred)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "\n",
    "        for more information. Use the learning rate from self.config.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                    Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def run_epoch(self, sess, inputs, labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session() object\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        n_minibatches, total_loss = 0, 0\n",
    "        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config.batch_size):\n",
    "            n_minibatches += 1\n",
    "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
    "        return total_loss / n_minibatches\n",
    "\n",
    "    def fit(self, sess, inputs, labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, inputs, labels)\n",
    "            duration = time.time() - start_time\n",
    "            print('Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration))\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 60.25 (0.043 sec)\n",
      "Epoch 1: loss = 21.24 (0.011 sec)\n",
      "Epoch 2: loss = 11.45 (0.011 sec)\n",
      "Epoch 3: loss = 7.65 (0.011 sec)\n",
      "Epoch 4: loss = 5.70 (0.010 sec)\n",
      "Epoch 5: loss = 4.53 (0.013 sec)\n",
      "Epoch 6: loss = 3.75 (0.012 sec)\n",
      "Epoch 7: loss = 3.19 (0.010 sec)\n",
      "Epoch 8: loss = 2.78 (0.011 sec)\n",
      "Epoch 9: loss = 2.46 (0.011 sec)\n",
      "Epoch 10: loss = 2.20 (0.011 sec)\n",
      "Epoch 11: loss = 2.00 (0.011 sec)\n",
      "Epoch 12: loss = 1.82 (0.011 sec)\n",
      "Epoch 13: loss = 1.68 (0.011 sec)\n",
      "Epoch 14: loss = 1.56 (0.010 sec)\n",
      "Epoch 15: loss = 1.45 (0.010 sec)\n",
      "Epoch 16: loss = 1.36 (0.012 sec)\n",
      "Epoch 17: loss = 1.27 (0.011 sec)\n",
      "Epoch 18: loss = 1.20 (0.011 sec)\n",
      "Epoch 19: loss = 1.14 (0.016 sec)\n",
      "Epoch 20: loss = 1.08 (0.020 sec)\n",
      "Epoch 21: loss = 1.03 (0.013 sec)\n",
      "Epoch 22: loss = 0.98 (0.011 sec)\n",
      "Epoch 23: loss = 0.93 (0.010 sec)\n",
      "Epoch 24: loss = 0.89 (0.010 sec)\n",
      "Epoch 25: loss = 0.86 (0.010 sec)\n",
      "Epoch 26: loss = 0.82 (0.010 sec)\n",
      "Epoch 27: loss = 0.79 (0.011 sec)\n",
      "Epoch 28: loss = 0.76 (0.010 sec)\n",
      "Epoch 29: loss = 0.74 (0.010 sec)\n",
      "Epoch 30: loss = 0.71 (0.012 sec)\n",
      "Epoch 31: loss = 0.69 (0.009 sec)\n",
      "Epoch 32: loss = 0.67 (0.009 sec)\n",
      "Epoch 33: loss = 0.65 (0.010 sec)\n",
      "Epoch 34: loss = 0.63 (0.010 sec)\n",
      "Epoch 35: loss = 0.61 (0.010 sec)\n",
      "Epoch 36: loss = 0.59 (0.011 sec)\n",
      "Epoch 37: loss = 0.58 (0.010 sec)\n",
      "Epoch 38: loss = 0.56 (0.010 sec)\n",
      "Epoch 39: loss = 0.55 (0.016 sec)\n",
      "Epoch 40: loss = 0.53 (0.018 sec)\n",
      "Epoch 41: loss = 0.52 (0.011 sec)\n",
      "Epoch 42: loss = 0.51 (0.010 sec)\n",
      "Epoch 43: loss = 0.49 (0.011 sec)\n",
      "Epoch 44: loss = 0.48 (0.010 sec)\n",
      "Epoch 45: loss = 0.47 (0.010 sec)\n",
      "Epoch 46: loss = 0.46 (0.010 sec)\n",
      "Epoch 47: loss = 0.45 (0.010 sec)\n",
      "Epoch 48: loss = 0.44 (0.009 sec)\n",
      "Epoch 49: loss = 0.43 (0.009 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_model():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    # Generate random data to train the model on\n",
    "    np.random.seed(1234)\n",
    "    inputs = np.random.rand(config.n_samples, config.n_features)\n",
    "    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n",
    "    labels[:, 0] = 1\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    # (not required but good practice)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # Build the model and add the variable initializer op\n",
    "        model = SoftmaxModel(config)\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    # Finalizing the graph causes tensorflow to raise an exception if you try to modify the graph\n",
    "    # further. This is good practice because it makes explicit the distinction between building and\n",
    "    # running the graph.\n",
    "    graph.finalize()\n",
    "\n",
    "    # Create a session for running ops in the graph\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # Run the op to initialize the variables.\n",
    "        sess.run(init_op)\n",
    "        # Fit the model\n",
    "        losses = model.fit(sess, inputs, labels)\n",
    "\n",
    "    # If ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print(\"Basic (non-exhaustive) classifier tests pass\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
