{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q2: Recurrent neural nets for NER\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from util import print_sentence, write_conll, read_conll\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper\n",
    "from ner_model import NERModel\n",
    "from defs import LBLS\n",
    "from q2_rnn_cell import RNNCell\n",
    "from q3_gru_cell import GRUCell\n",
    "\n",
    "logger = logging.getLogger(\"hw3.q2\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_word_features = 2 # Number of features for every word in the input.\n",
    "    window_size = 1\n",
    "    n_features = (2 * window_size + 1) * n_word_features # Number of features for every word in the input.\n",
    "    max_length = 120 # longest sequence to parse\n",
    "    n_classes = 5\n",
    "    dropout = 0.5\n",
    "    embed_size = 50\n",
    "    hidden_size = 300\n",
    "    batch_size = 32\n",
    "    n_epochs = 10\n",
    "    max_grad_norm = 10.\n",
    "    lr = 0.001\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.cell = args.cell\n",
    "\n",
    "        if \"model_path\" in args:\n",
    "            # Where to save things.\n",
    "            self.output_path = args.model_path\n",
    "        else:\n",
    "            self.output_path = \"results/{}/{:%Y%m%d_%H%M%S}/\".format(self.cell, datetime.now())\n",
    "        self.model_output = self.output_path + \"model.weights\"\n",
    "        self.eval_output = self.output_path + \"results.txt\"\n",
    "        self.conll_output = self.output_path + \"{}_predictions.conll\".format(self.cell)\n",
    "        self.log_output = self.output_path + \"log\"\n",
    "\n",
    "def pad_sequences(data, max_length):\n",
    "    \"\"\"Ensures each input-output seqeunce pair in @data is of length\n",
    "    @max_length by padding it with zeros and truncating the rest of the\n",
    "    sequence.\n",
    "\n",
    "    TODO: In the code below, for every sentence, labels pair in @data,\n",
    "    (a) create a new sentence which appends zero feature vectors until\n",
    "    the sentence is of length @max_length. If the sentence is longer\n",
    "    than @max_length, simply truncate the sentence to be @max_length\n",
    "    long.\n",
    "    (b) create a new label sequence similarly.\n",
    "    (c) create a _masking_ sequence that has a True wherever there was a\n",
    "    token in the original sequence, and a False for every padded input.\n",
    "\n",
    "    Example: for the (sentence, labels) pair: [[4,1], [6,0], [7,0]], [1,\n",
    "    0, 0], and max_length = 5, we would construct\n",
    "        - a new sentence: [[4,1], [6,0], [7,0], [0,0], [0,0]]\n",
    "        - a new label seqeunce: [1, 0, 0, 4, 4], and\n",
    "        - a masking seqeunce: [True, True, True, False, False].\n",
    "\n",
    "    Args:\n",
    "        data: is a list of (sentence, labels) tuples. @sentence is a list\n",
    "            containing the words in the sentence and @label is a list of\n",
    "            output labels. Each word is itself a list of\n",
    "            @n_features features. For example, the sentence \"Chris\n",
    "            Manning is amazing\" and labels \"PER PER O O\" would become\n",
    "            ([[1,9], [2,9], [3,8], [4,8]], [1, 1, 4, 4]). Here \"Chris\"\n",
    "            the word has been featurized as \"[1, 9]\", and \"[1, 1, 4, 4]\"\n",
    "            is the list of labels. \n",
    "        max_length: the desired length for all input/output sequences.\n",
    "    Returns:\n",
    "        a new list of data points of the structure (sentence', labels', mask).\n",
    "        Each of sentence', labels' and mask are of length @max_length.\n",
    "        See the example above for more details.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "\n",
    "    # Use this zero vector when padding sequences.\n",
    "    zero_vector = [0] * Config.n_features\n",
    "    zero_label = 4 # corresponds to the 'O' tag\n",
    "\n",
    "    for sentence, labels in data:\n",
    "        ### YOUR CODE HERE (~4-6 lines)\n",
    "        d = max_length-len(sentence)\n",
    "        mask = [True]*len(sentence)+[False]*d if d>0 else [True]*max_length\n",
    "        sentence = sentence+[zero_vector]*d if d>0 else sentence[0:max_length]\n",
    "        labels = labels+[zero_label]*d if d>0 else labels[0:max_length]\n",
    "\n",
    "        \n",
    "        ret.append((sentence,labels,mask))\n",
    "        ### END YOUR CODE ###\n",
    "    return ret\n",
    "\n",
    "class RNNModel(NERModel):\n",
    "    \"\"\"\n",
    "    Implements a recursive neural network with an embedding layer and\n",
    "    single hidden layer.\n",
    "    This network will predict a sequence of labels (e.g. PER) for a\n",
    "    given token (e.g. Henry) using a featurized window around the token.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
    "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
    "        (so we can use different batch sizes without rebuilding the model).\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of  shape (None, self.max_length, n_features), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape (None, self.max_length), type tf.int32\n",
    "        mask_placeholder:  Mask placeholder tensor of shape (None, self.max_length), type tf.bool\n",
    "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
    "\n",
    "        TODO: Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "            self.mask_placeholder\n",
    "            self.dropout_placeholder\n",
    "\n",
    "        HINTS:\n",
    "            - Remember to use self.max_length NOT Config.max_length\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~4-6 lines)\n",
    "        self.input_placeholder = tf.placeholder(tf.int32, shape=(None, self.max_length,self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, shape=(None,self.max_length))\n",
    "        self.mask_placeholder = tf.placeholder(tf.bool,shape=(None, self.max_length))\n",
    "        self.dropout_placeholder  = tf.placeholder(tf.float32)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, mask_batch, labels_batch=None, dropout=1):\n",
    "        \"\"\"Creates the feed_dict for the dependency parser.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "        Hint: When an argument is None, don't add it to the feed_dict.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            mask_batch:   A batch of mask data.\n",
    "            labels_batch: A batch of label data.\n",
    "            dropout: The dropout rate.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE (~6-10 lines)\n",
    "        feed_dict = {self.input_placeholder:inputs_batch,self.mask_placeholder:mask_batch,self.dropout_placeholder: dropout}\n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
    "        concatenates those vectors:\n",
    "\n",
    "        TODO:\n",
    "            - Create an embedding tensor and initialize it with self.pretrained_embeddings.\n",
    "            - Use the input_placeholder to index into the embeddings tensor, resulting in a\n",
    "              tensor of shape (None, max_length, n_features, embed_size).\n",
    "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
    "              (None, max_length, n_features * embed_size).\n",
    "\n",
    "        HINTS:\n",
    "            - You might find tf.nn.embedding_lookup useful.\n",
    "            - You can use tf.reshape to concatenate the vectors. See\n",
    "              following link to understand what -1 in a shape means.\n",
    "              https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n",
    "\n",
    "        Returns:\n",
    "            embeddings: tf.Tensor of shape (None, max_length, n_features*embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~4-6 lines)\n",
    "        embeddings = tf.Variable(self.pretrained_embeddings)\n",
    "        embeddings = tf.nn.embedding_lookup(embeddings,self.input_placeholder)\n",
    "        embeddings = tf.reshape(embeddings,(-1,self.max_length,self.config.n_features*self.config.embed_size))\n",
    "        ### END YOUR CODE\n",
    "        return embeddings\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the unrolled RNN:\n",
    "            h_0 = 0\n",
    "            for t in 1 to T:\n",
    "                o_t, h_t = cell(x_t, h_{t-1})\n",
    "                o_drop_t = Dropout(o_t, dropout_rate)\n",
    "                y_t = o_drop_t U + b_2\n",
    "\n",
    "        TODO: There a quite a few things you'll need to do in this function:\n",
    "            - Define the variables U, b_2.\n",
    "            - Define the vector h as a constant and inititalize it with\n",
    "              zeros. See tf.zeros and tf.shape for information on how\n",
    "              to initialize this variable to be of the right shape.\n",
    "              https://www.tensorflow.org/api_docs/python/tf/zeros\n",
    "              https://www.tensorflow.org/api_docs/python/tf/shape\n",
    "            - In a for loop, begin to unroll the RNN sequence. Collect\n",
    "              the predictions in a list.\n",
    "            - When unrolling the loop, from the second iteration\n",
    "              onwards, you will HAVE to call\n",
    "              tf.get_variable_scope().reuse_variables() so that you do\n",
    "              not create new variables in the RNN cell.\n",
    "              See https://www.tensorflow.org/api_guides/python/state_ops#Sharing_Variables\n",
    "            - Concatenate and reshape the predictions into a predictions\n",
    "              tensor.\n",
    "        Hint: You will find the function tf.stack (similar to np.asarray)\n",
    "              useful to assemble a list of tensors into a larger tensor.\n",
    "              https://www.tensorflow.org/api_docs/python/tf/stack\n",
    "        Hint: You will find the function tf.transpose and the perms\n",
    "              argument useful to shuffle the indices of the tensor.\n",
    "              https://www.tensorflow.org/api_docs/python/tf/transpose\n",
    "\n",
    "        Remember:\n",
    "            * Use the xavier initilization for matrices.\n",
    "            * Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n",
    "            The keep probability should be set to the value of self.dropout_placeholder\n",
    "\n",
    "        Returns:\n",
    "            pred: tf.Tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.add_embedding()\n",
    "        dropout_rate = self.dropout_placeholder\n",
    "\n",
    "        preds = [] # Predicted output at each timestep should go here!\n",
    "\n",
    "        # Use the cell defined below. For Q2, we will just be using the\n",
    "        # RNNCell you defined, but for Q3, we will run this code again\n",
    "        # with a GRU cell!\n",
    "        if self.config.cell == \"rnn\":\n",
    "            cell = RNNCell(Config.n_features * Config.embed_size, Config.hidden_size)\n",
    "        elif self.config.cell == \"gru\":\n",
    "            cell = GRUCell(Config.n_features * Config.embed_size, Config.hidden_size)\n",
    "        else:\n",
    "            raise ValueError(\"Unsuppported cell type: \" + self.config.cell)\n",
    "\n",
    "        # Define U and b2 as variables.\n",
    "        # Initialize state as vector of zeros.\n",
    "        ### YOUR CODE HERE (~4-6 lines)\n",
    "        U = tf.get_variable(\"U\",dtype=tf.float32,shape=(self.config.hidden_size,self.config.n_classes),initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.Variable(tf.zeros((self.config.n_classes,)), name='b2')\n",
    "        h_0 = tf.zeros((1, self.config.hidden_size), name='h0')\n",
    "        h_t = h_0\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(self.max_length):\n",
    "                ### YOUR CODE HERE (~6-10 lines)\n",
    "                if time_step>0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                o_t, h_t = cell(x[:,time_step], h_t,'RNN')\n",
    "                o_drop_t = tf.nn.dropout(o_t, dropout_rate)\n",
    "                y_t = tf.matmul(o_drop_t,U) + b2\n",
    "                preds.append(y_t)\n",
    "                ### END YOUR CODE\n",
    "\n",
    "        # Make sure to reshape @preds here.\n",
    "        ### YOUR CODE HERE (~2-4 lines)\n",
    "        preds = tf.stack(preds,axis=1)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        assert preds.get_shape().as_list() == [None, self.max_length, self.config.n_classes], \"predictions are not of the right shape. Expected {}, got {}\".format([None, self.max_length, self.config.n_classes], preds.get_shape().as_list())\n",
    "        return preds\n",
    "\n",
    "    def add_loss_op(self, preds):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "\n",
    "        TODO: Compute averaged cross entropy loss for the predictions.\n",
    "        Importantly, you must ignore the loss for any masked tokens.\n",
    "\n",
    "        Hint: You might find tf.boolean_mask useful to mask the losses on masked tokens.\n",
    "        Hint: You can use tf.nn.sparse_softmax_cross_entropy_with_logits to simplify your\n",
    "                    implementation. You might find tf.reduce_mean useful.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, max_length, n_classes) containing the output of the neural\n",
    "                  network before the softmax layer.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~2-4 lines)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels_placeholder, logits=preds)\n",
    "        loss = tf.reduce_mean(tf.boolean_mask(loss,self.mask_placeholder))\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Use tf.train.AdamOptimizer for this model.\n",
    "        Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-2 lines)\n",
    "        train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def preprocess_sequence_data(self, examples):\n",
    "        def featurize_windows(data, start, end, window_size = 1):\n",
    "            \"\"\"Uses the input sequences in @data to construct new windowed data points.\n",
    "            \"\"\"\n",
    "            ret = []\n",
    "            for sentence, labels in data:\n",
    "                from util import window_iterator\n",
    "                sentence_ = []\n",
    "                for window in window_iterator(sentence, window_size, beg=start, end=end):\n",
    "                    sentence_.append(sum(window, []))\n",
    "                ret.append((sentence_, labels))\n",
    "            return ret\n",
    "\n",
    "        examples = featurize_windows(examples, self.helper.START, self.helper.END)\n",
    "        return pad_sequences(examples, self.max_length)\n",
    "\n",
    "    def consolidate_predictions(self, examples_raw, examples, preds):\n",
    "        \"\"\"Batch the predictions into groups of sentence length.\n",
    "        \"\"\"\n",
    "        assert len(examples_raw) == len(examples)\n",
    "        assert len(examples_raw) == len(preds)\n",
    "\n",
    "        ret = []\n",
    "        for i, (sentence, labels) in enumerate(examples_raw):\n",
    "            _, _, mask = examples[i]\n",
    "            labels_ = [l for l, m in zip(preds[i], mask) if m] # only select elements of mask.\n",
    "            assert len(labels_) == len(labels)\n",
    "            ret.append([sentence, labels, labels_])\n",
    "        return ret\n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch, mask_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch=inputs_batch, mask_batch=mask_batch)\n",
    "        predictions = sess.run(tf.argmax(self.pred, axis=2), feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch, mask_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch, mask_batch=mask_batch,\n",
    "                                     dropout=Config.dropout)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def __init__(self, helper, config, pretrained_embeddings, report=None):\n",
    "        super(RNNModel, self).__init__(helper, config, report)\n",
    "        self.max_length = min(Config.max_length, helper.max_length)\n",
    "        Config.max_length = self.max_length # Just in case people make a mistake.\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "\n",
    "        # Defining placeholders.\n",
    "        self.input_placeholder = None\n",
    "        self.labels_placeholder = None\n",
    "        self.mask_placeholder = None\n",
    "        self.dropout_placeholder = None\n",
    "\n",
    "        self.build()\n",
    "\n",
    "def test_pad_sequences():\n",
    "    Config.n_features = 2\n",
    "    data = [\n",
    "        ([[4,1], [6,0], [7,0]], [1, 0, 0]),\n",
    "        ([[3,0], [3,4], [4,5], [5,3], [3,4]], [0, 1, 0, 2, 3]),\n",
    "        ]\n",
    "    ret = [\n",
    "        ([[4,1], [6,0], [7,0], [0,0]], [1, 0, 0, 4], [True, True, True, False]),\n",
    "        ([[3,0], [3,4], [4,5], [5,3]], [0, 1, 0, 2], [True, True, True, True])\n",
    "        ]\n",
    "\n",
    "    ret_ = pad_sequences(data, 4)\n",
    "    assert len(ret_) == 2, \"Did not process all examples: expected {} results, but got {}.\".format(2, len(ret_))\n",
    "    for i in range(2):\n",
    "        assert len(ret_[i]) == 3, \"Did not populate return values corrected: expected {} items, but got {}.\".format(3, len(ret_[i]))\n",
    "        for j in range(3):\n",
    "            assert ret_[i][j] == ret[i][j], \"Expected {}, but got {} for {}-th entry of {}-th example\".format(ret[i][j], ret_[i][j], j, i)\n",
    "\n",
    "def do_test1(_):\n",
    "    logger.info(\"Testing pad_sequences\")\n",
    "    test_pad_sequences()\n",
    "    logger.info(\"Passed!\")\n",
    "\n",
    "def do_test2(args):\n",
    "    logger.info(\"Testing implementation of RNNModel\")\n",
    "    config = Config(args)\n",
    "    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = RNNModel(helper, config, embeddings)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = None\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            model.fit(session, saver, train, dev)\n",
    "\n",
    "    logger.info(\"Model did not crash!\")\n",
    "    logger.info(\"Passed!\")\n",
    "\n",
    "def do_train(args):\n",
    "    # Set up some parameters.\n",
    "    config = Config(args)\n",
    "    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "    helper.save(config.output_path)\n",
    "\n",
    "    handler = logging.FileHandler(config.log_output)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "    logging.getLogger().addHandler(handler)\n",
    "\n",
    "    report = None #Report(Config.eval_output)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = RNNModel(helper, config, embeddings)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            model.fit(session, saver, train, dev)\n",
    "            if report:\n",
    "                report.log_output(model.output(session, dev_raw))\n",
    "                report.save()\n",
    "            else:\n",
    "                # Save predictions in a text file.\n",
    "                output = model.output(session, dev_raw)\n",
    "                sentences, labels, predictions = zip(*output)\n",
    "                predictions = [[LBLS[l] for l in preds] for preds in predictions]\n",
    "                output = zip(sentences, labels, predictions)\n",
    "\n",
    "                with open(model.config.conll_output, 'w') as f:\n",
    "                    write_conll(f, output)\n",
    "                with open(model.config.eval_output, 'w') as f:\n",
    "                    for sentence, labels, predictions in output:\n",
    "                        print_sentence(f, sentence, labels, predictions)\n",
    "\n",
    "def do_evaluate(args):\n",
    "    config = Config(args)\n",
    "    helper = ModelHelper.load(args.model_path)\n",
    "    input_data = read_conll(args.data)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = RNNModel(helper, config, embeddings)\n",
    "\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            saver.restore(session, model.config.model_output)\n",
    "            for sentence, labels, predictions in model.output(session, input_data):\n",
    "                predictions = [LBLS[l] for l in predictions]\n",
    "                print_sentence(args.output, sentence, labels, predictions)\n",
    "\n",
    "def do_shell(args):\n",
    "    config = Config(args)\n",
    "    helper = ModelHelper.load(args.model_path)\n",
    "    embeddings = load_embeddings(args, helper)\n",
    "    config.embed_size = embeddings.shape[1]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logger.info(\"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = RNNModel(helper, config, embeddings)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            saver.restore(session, model.config.model_output)\n",
    "\n",
    "            print(\"\"\"Welcome!\n",
    "You can use this shell to explore the behavior of your model.\n",
    "Please enter sentences with spaces between tokens, e.g.,\n",
    "input> Germany 's representative to the European Union 's veterinary committee .\n",
    "\"\"\")\n",
    "            while True:\n",
    "                # Create simple REPL\n",
    "                try:\n",
    "                    sentence = raw_input(\"input> \")\n",
    "                    tokens = sentence.strip().split(\" \")\n",
    "                    for sentence, _, predictions in model.output(session, [(tokens, [\"O\"] * len(tokens))]):\n",
    "                        predictions = [LBLS[l] for l in predictions]\n",
    "                        print_sentence(sys.stdout, sentence, [\"\"] * len(tokens), predictions)\n",
    "                except EOFError:\n",
    "                    print(\"Closing session.\")\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Trains and tests an NER model')\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    command_parser = subparsers.add_parser('test1', help='')\n",
    "    command_parser.set_defaults(func=do_test1)\n",
    "\n",
    "    command_parser = subparsers.add_parser('test2', help='')\n",
    "    command_parser.add_argument('-dt', '--data-train', type=argparse.FileType('r'), default=\"data/tiny.conll\", help=\"Training data\")\n",
    "    command_parser.add_argument('-dd', '--data-dev', type=argparse.FileType('r'), default=\"data/tiny.conll\", help=\"Dev data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\", help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\", help=\"Path to word vectors file\")\n",
    "    command_parser.add_argument('-c', '--cell', choices=[\"rnn\", \"gru\"], default=\"rnn\", help=\"Type of RNN cell to use.\")\n",
    "    command_parser.set_defaults(func=do_test2)\n",
    "\n",
    "    command_parser = subparsers.add_parser('train', help='')\n",
    "    command_parser.add_argument('-dt', '--data-train', type=argparse.FileType('r'), default=\"data/train.conll\", help=\"Training data\")\n",
    "    command_parser.add_argument('-dd', '--data-dev', type=argparse.FileType('r'), default=\"data/dev.conll\", help=\"Dev data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\", help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\", help=\"Path to word vectors file\")\n",
    "    command_parser.add_argument('-c', '--cell', choices=[\"rnn\", \"gru\"], default=\"rnn\", help=\"Type of RNN cell to use.\")\n",
    "    command_parser.set_defaults(func=do_train)\n",
    "\n",
    "    command_parser = subparsers.add_parser('evaluate', help='')\n",
    "    command_parser.add_argument('-d', '--data', type=argparse.FileType('r'), default=\"data/dev.conll\", help=\"Training data\")\n",
    "    command_parser.add_argument('-m', '--model-path', help=\"Training data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\", help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\", help=\"Path to word vectors file\")\n",
    "    command_parser.add_argument('-c', '--cell', choices=[\"rnn\", \"gru\"], default=\"rnn\", help=\"Type of RNN cell to use.\")\n",
    "    command_parser.add_argument('-o', '--output', type=argparse.FileType('w'), default=sys.stdout, help=\"Training data\")\n",
    "    command_parser.set_defaults(func=do_evaluate)\n",
    "\n",
    "    command_parser = subparsers.add_parser('shell', help='')\n",
    "    command_parser.add_argument('-m', '--model-path', help=\"Training data\")\n",
    "    command_parser.add_argument('-v', '--vocab', type=argparse.FileType('r'), default=\"data/vocab.txt\", help=\"Path to vocabulary file\")\n",
    "    command_parser.add_argument('-vv', '--vectors', type=argparse.FileType('r'), default=\"data/wordVectors.txt\", help=\"Path to word vectors file\")\n",
    "    command_parser.add_argument('-c', '--cell', choices=[\"rnn\", \"gru\"], default=\"rnn\", help=\"Type of RNN cell to use.\")\n",
    "    command_parser.set_defaults(func=do_shell)\n",
    "\n",
    "    ARGS = parser.parse_args()\n",
    "    if ARGS.func is None:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        ARGS.func(ARGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lhd0430/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "INFO:Testing pad_sequences\n",
      "INFO:Passed!\n"
     ]
    }
   ],
   "source": [
    "!python q2_rnn.py test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[True]*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lhd0430/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "INFO:Testing implementation of RNNModel\n",
      "INFO:Loading training data...\n",
      "INFO:Done. Read 721 sentences\n",
      "INFO:Loading dev data...\n",
      "INFO:Done. Read 721 sentences\n",
      "INFO:Built dictionary for 2653 features.\n",
      "INFO:Initialized embeddings.\n",
      "INFO:Building model...\n",
      "/Users/lhd0430/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "INFO:took 12.63 seconds\n",
      "INFO:Epoch 1 out of 10\n",
      "Traceback (most recent call last):\n",
      "  File \"q2_rnn.py\", line 589, in <module>\n",
      "    ARGS.func(ARGS)\n",
      "  File \"q2_rnn.py\", line 437, in do_test2\n",
      "    model.fit(session, saver, train, dev)\n",
      "  File \"/Users/lhd0430/Documents/DataLearning/DeepLearning/NPL-stanford/assignment3/ner_model.py\", line 116, in fit\n",
      "    loss = self.train_on_batch(sess, *batch)\n",
      "  File \"q2_rnn.py\", line 379, in train_on_batch\n",
      "    _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
      "  File \"/Users/lhd0430/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/Users/lhd0430/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1076, in _run\n",
      "    str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (32,) for Tensor 'Placeholder_2:0', which has shape '(?, 52)'\n"
     ]
    }
   ],
   "source": [
    "!python q2_rnn.py test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
