{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Q3: Grooving with GRUs\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import Progbar, minibatches\n",
    "from model import Model\n",
    "\n",
    "from q3_gru_cell import GRUCell\n",
    "from q2_rnn_cell import RNNCell\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "logger = logging.getLogger(\"hw3.q3\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation. Use self.config.? instead of Config.?\n",
    "    \"\"\"\n",
    "    max_length = 20 # Length of sequence used.\n",
    "    batch_size = 100\n",
    "    n_epochs = 40\n",
    "    lr = 0.2\n",
    "    max_grad_norm = 5.\n",
    "\n",
    "class SequencePredictor(Model):\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "        NOTE: You do not have to do anything here.\n",
    "        \"\"\"\n",
    "        self.inputs_placeholder = tf.placeholder(tf.float32, shape=(None, self.config.max_length, 1), name=\"x\")\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for the model.\n",
    "        NOTE: You do not have to do anything here.\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.inputs_placeholder: inputs_batch,\n",
    "            }\n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Runs an rnn on the input using TensorFlows's\n",
    "        @tf.nn.dynamic_rnn function, and returns the final state as a prediction.\n",
    "\n",
    "        TODO:\n",
    "            - Call tf.nn.dynamic_rnn using @cell below. See:\n",
    "              https://www.tensorflow.org/api_docs/python/nn/recurrent_neural_networks\n",
    "            - Apply a sigmoid transformation on the final state to\n",
    "              normalize the inputs between 0 and 1.\n",
    "\n",
    "        Returns:\n",
    "            preds: tf.Tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick out the cell to use here.\n",
    "        if self.config.cell == \"rnn\":\n",
    "            cell = RNNCell(1, 1)\n",
    "        elif self.config.cell == \"gru\":\n",
    "            cell = GRUCell(1, 1)\n",
    "        elif self.config.cell == \"lstm\":\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported cell type.\")\n",
    "\n",
    "        x = self.inputs_placeholder\n",
    "        ### YOUR CODE HERE (~2-3 lines)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x,dtype=tf.float32)\n",
    "        preds = tf.sigmoid(state)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return preds #state # preds\n",
    "\n",
    "    def add_loss_op(self, preds):\n",
    "        \"\"\"Adds ops to compute the loss function.\n",
    "        Here, we will use a simple l2 loss.\n",
    "\n",
    "        Tips:\n",
    "            - You may find the functions tf.reduce_mean and tf.l2_loss\n",
    "              useful.\n",
    "\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, 1) containing the last\n",
    "            state of the neural network.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        y = self.labels_placeholder\n",
    "\n",
    "        ### YOUR CODE HERE (~1-2 lines)\n",
    "        loss = tf.nn.l2_loss(y-preds)\n",
    "        loss= tf.reduce_mean(loss)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        TODO:\n",
    "            - Get the gradients for the loss from optimizer using\n",
    "              optimizer.compute_gradients.\n",
    "            - if self.clip_gradients is true, clip the global norm of\n",
    "              the gradients using tf.clip_by_global_norm to self.config.max_grad_norm\n",
    "            - Compute the resultant global norm of the gradients using\n",
    "              tf.global_norm and save this global norm in self.grad_norm.\n",
    "            - Finally, actually create the training operation by calling\n",
    "              optimizer.apply_gradients.\n",
    "\t\t\t- Remember to clip gradients only if self.config.clip_gradients\n",
    "\t\t\t  is True.\n",
    "\t\t\t- Remember to set self.grad_norm\n",
    "        See: https://www.tensorflow.org/api_docs/python/train/gradient_clipping\n",
    "        Args:\n",
    "            loss: Loss tensor.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.config.lr)\n",
    "\n",
    "        ### YOUR CODE HERE (~6-10 lines)\n",
    "        grad_variable_pairs = optimizer.compute_gradients(loss)\n",
    "        grad,variables = tuple(zip(*grad_variable_pairs))\n",
    "\n",
    "        # - Remember to clip gradients only if self.config.clip_gradients\n",
    "        # is True.\n",
    "        if self.config.clip_gradients is True:\n",
    "            grad, self.grad_norm = tf.clip_by_global_norm(grad,self.config.max_grad_norm)\n",
    "        else:        \n",
    "        # - Remember to set self.grad_norm\n",
    "            self.grad_norm = tf.global_norm(grad)\n",
    "        \n",
    "        train_op = optimizer.apply_gradients(zip(grad,variables))\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        assert self.grad_norm is not None, \"grad_norm was not set properly!\"\n",
    "        return train_op\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        \"\"\"Perform one step of gradient descent on the provided batch of data.\n",
    "        This version also returns the norm of gradients.\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n",
    "        _, loss, grad_norm = sess.run([self.train_op, self.loss, self.grad_norm], feed_dict=feed)\n",
    "        return loss, grad_norm\n",
    "\n",
    "    def run_epoch(self, sess, train):\n",
    "        prog = Progbar(target=1 + int(len(train) / self.config.batch_size))\n",
    "        losses, grad_norms = [], []\n",
    "        for i, batch in enumerate(minibatches(train, self.config.batch_size)):\n",
    "            loss, grad_norm = self.train_on_batch(sess, *batch)\n",
    "            losses.append(loss)\n",
    "            grad_norms.append(grad_norm)\n",
    "            prog.update(i + 1, [(\"train loss\", loss)])\n",
    "\n",
    "        return losses, grad_norms\n",
    "\n",
    "    def fit(self, sess, train):\n",
    "        losses, grad_norms = [], []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            logger.info(\"Epoch %d out of %d\", epoch + 1, self.config.n_epochs)\n",
    "            loss, grad_norm = self.run_epoch(sess, train)\n",
    "            losses.append(loss)\n",
    "            grad_norms.append(grad_norm)\n",
    "\n",
    "        return losses, grad_norms\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.inputs_placeholder = None\n",
    "        self.labels_placeholder = None\n",
    "        self.grad_norm = None\n",
    "        self.build()\n",
    "\n",
    "def generate_sequence(max_length=20, n_samples=9999):\n",
    "    \"\"\"\n",
    "    Generates a sequence like a [0]*n a\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for _ in range(int(n_samples/2)):\n",
    "        seqs.append(([[0.,]] + ([[0.,]] * (max_length-1)), [0.]))\n",
    "        seqs.append(([[1.,]] + ([[0.,]] * (max_length-1)), [1.]))\n",
    "    return seqs\n",
    "\n",
    "def test_generate_sequence():\n",
    "    max_length = 20\n",
    "    for seq, y in generate_sequence(20):\n",
    "        assert len(seq) == max_length\n",
    "        assert seq[0] == y\n",
    "\n",
    "def make_dynamics_plot(args, x, h, ht_rnn, ht_gru, params):\n",
    "    matplotlib.rc('text', usetex=True)\n",
    "    matplotlib.rc('font', family='serif')\n",
    "\n",
    "    Ur, Wr, br, Uz, Wz, bz, Uo, Wo, bo = params\n",
    "\n",
    "    plt.clf()\n",
    "    plt.title(\"\"\"Cell dynamics when x={}:\n",
    "Ur={:.2f}, Wr={:.2f}, br={:.2f}\n",
    "Uz={:.2f}, Wz={:.2f}, bz={:.2f}\n",
    "Uo={:.2f}, Wo={:.2f}, bo={:.2f}\"\"\".format(x, Ur[0,0], Wr[0,0], br[0], Uz[0,0], Wz[0,0], bz[0], Uo[0,0], Wo[0,0], bo[0]))\n",
    "\n",
    "    plt.plot(h, ht_rnn, label=\"rnn\")\n",
    "    plt.plot(h, ht_gru, label=\"gru\")\n",
    "    plt.plot(h, h, color='gray', linestyle='--')\n",
    "    plt.ylabel(\"$h_{t}$\")\n",
    "    plt.xlabel(\"$h_{t-1}$\")\n",
    "    plt.legend()\n",
    "    output_path = \"{}-{}-{}.png\".format(args.output_prefix, x, \"dynamics\")\n",
    "    plt.savefig(output_path)\n",
    "\n",
    "def compute_cell_dynamics(args):\n",
    "    with tf.Graph().as_default():\n",
    "        # You can change this around, but make sure to reset it to 41 when\n",
    "        # submitting.\n",
    "        np.random.seed(41)\n",
    "        tf.set_random_seed(41)\n",
    "\n",
    "        with tf.variable_scope(\"dynamics\"):\n",
    "            x_placeholder = tf.placeholder(tf.float32, shape=(None,1))\n",
    "            h_placeholder = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "            def mat(x):\n",
    "                return np.atleast_2d(np.array(x, dtype=np.float32))\n",
    "            def vec(x):\n",
    "                return np.atleast_1d(np.array(x, dtype=np.float32))\n",
    "\n",
    "            with tf.variable_scope(\"cell\"):\n",
    "                Ur, Wr, Uz, Wz, Uo, Wo = [mat(3*x) for x in np.random.randn(6)]\n",
    "                br, bz, bo = [vec(x) for x in np.random.randn(3)]\n",
    "                params = [Ur, Wr, br, Uz, Wz, bz, Uo, Wo, bo]\n",
    "\n",
    "                tf.get_variable(\"W_r\", initializer=Wr)\n",
    "                tf.get_variable(\"U_r\", initializer=Ur)\n",
    "                tf.get_variable(\"b_r\", initializer=br)\n",
    "\n",
    "                tf.get_variable(\"W_z\", initializer=Wz)\n",
    "                tf.get_variable(\"U_z\", initializer=Uz)\n",
    "                tf.get_variable(\"b_z\", initializer=bz)\n",
    "\n",
    "                tf.get_variable(\"W_o\", initializer=Wo)\n",
    "                tf.get_variable(\"U_o\", initializer=Uo)\n",
    "                tf.get_variable(\"b_o\", initializer=bo)\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            y_gru, h_gru = GRUCell(1,1)(x_placeholder, h_placeholder, scope=\"cell\")\n",
    "            y_rnn, h_rnn = GRUCell(1,1)(x_placeholder, h_placeholder, scope=\"cell\")\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as session:\n",
    "                session.run(init)\n",
    "\n",
    "                x = mat(np.zeros(1000)).T\n",
    "                h = mat(np.linspace(-3, 3, 1000)).T\n",
    "                ht_gru = session.run([h_gru], feed_dict={x_placeholder: x, h_placeholder: h})\n",
    "                ht_rnn = session.run([h_rnn], feed_dict={x_placeholder: x, h_placeholder: h})\n",
    "                ht_gru = np.array(ht_gru)[0]\n",
    "                ht_rnn = np.array(ht_rnn)[0]\n",
    "                make_dynamics_plot(args, 0, h, ht_rnn, ht_gru, params)\n",
    "\n",
    "                x = mat(np.ones(1000)).T\n",
    "                h = mat(np.linspace(-3, 3, 1000)).T\n",
    "                ht_gru = session.run([h_gru], feed_dict={x_placeholder: x, h_placeholder: h})\n",
    "                ht_rnn = session.run([h_rnn], feed_dict={x_placeholder: x, h_placeholder: h})\n",
    "                ht_gru = np.array(ht_gru)[0]\n",
    "                ht_rnn = np.array(ht_rnn)[0]\n",
    "                make_dynamics_plot(args, 1, h, ht_rnn, ht_gru, params)\n",
    "\n",
    "def make_prediction_plot(args, losses, grad_norms):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"{} on sequences of length {} ({} gradient clipping)\".format(args.cell, args.max_length, \"with\" if args.clip_gradients else \"without\"))\n",
    "    plt.plot(np.arange(losses.size), losses.flatten(), label=\"Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(np.arange(grad_norms.size), grad_norms.flatten(), label=\"Gradients\")\n",
    "    plt.ylabel(\"Gradients\")\n",
    "    plt.xlabel(\"Minibatch\")\n",
    "    output_path = \"{}-{}clip-{}.png\".format(args.output_prefix, \"\" if args.clip_gradients else \"no\", args.cell)\n",
    "    plt.savefig(output_path)\n",
    "\n",
    "def do_sequence_prediction(args):\n",
    "    # Set up some parameters.\n",
    "    config = Config()\n",
    "    config.cell = args.cell\n",
    "    config.clip_gradients = args.clip_gradients\n",
    "\n",
    "    # You can change this around, but make sure to reset it to 41 when\n",
    "    # submitting.\n",
    "    np.random.seed(41)\n",
    "    data = generate_sequence(args.max_length)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        # You can change this around, but make sure to reset it to 41 when\n",
    "        # submitting.\n",
    "        tf.set_random_seed(59)\n",
    "\n",
    "        # Initializing RNNs weights to be very large to showcase\n",
    "        # gradient clipping.\n",
    "\n",
    "\n",
    "        logger.info(\"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = SequencePredictor(config)\n",
    "        logger.info(\"took %.2f seconds\", time.time() - start)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            losses, grad_norms = model.fit(session, data)\n",
    "\n",
    "    # Plotting code.\n",
    "    losses, grad_norms = np.array(losses), np.array(grad_norms)\n",
    "    make_prediction_plot(args, losses, grad_norms)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Runs a sequence model to test latching behavior of memory, e.g. 100000000 -> 1')\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    command_parser = subparsers.add_parser('predict', help='Plot prediction behavior of different cells')\n",
    "    command_parser.add_argument('-c', '--cell', choices=['rnn', 'gru', 'lstm'], default='rnn', help=\"Type of cell to use\")\n",
    "    command_parser.add_argument('-g', '--clip_gradients', action='store_true', default=False, help=\"If true, clip gradients\")\n",
    "    command_parser.add_argument('-l', '--max-length', type=int, default=20, help=\"Length of sequences to generate\")\n",
    "    command_parser.add_argument('-o', '--output-prefix', type=str, default=\"q3\", help=\"Length of sequences to generate\")\n",
    "    command_parser.set_defaults(func=do_sequence_prediction)\n",
    "\n",
    "    # Easter egg! Run this function to plot how an RNN or GRU map an\n",
    "    # input state to an output state.\n",
    "    command_parser = subparsers.add_parser('dynamics', help=\"Plot cell's dynamics\")\n",
    "    command_parser.add_argument('-o', '--output-prefix', type=str, default=\"q3\", help=\"Length of sequences to generate\")\n",
    "    command_parser.set_defaults(func=compute_cell_dynamics)\n",
    "\n",
    "\n",
    "    ARGS = parser.parse_args()\n",
    "    if ARGS.func is None:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        ARGS.func(ARGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lhd0430/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "q3_gru.py:28: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"q3_gru.py\", line 20, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/Users/lhd0430/anaconda/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/Users/lhd0430/anaconda/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('TkAgg')\n",
      "INFO:Building model...\n",
      "INFO:took 9.35 seconds\n",
      "INFO:Epoch 1 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5200     \n",
      "INFO:Epoch 2 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5330     \n",
      "INFO:Epoch 3 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5131     \n",
      "INFO:Epoch 4 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5176     \n",
      "INFO:Epoch 5 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5234     \n",
      "INFO:Epoch 6 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5211     \n",
      "INFO:Epoch 7 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5135     \n",
      "INFO:Epoch 8 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5136     \n",
      "INFO:Epoch 9 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5116     \n",
      "INFO:Epoch 10 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5117     \n",
      "INFO:Epoch 11 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5078     \n",
      "INFO:Epoch 12 out of 40\n",
      "100/100 [==============================] - 1s - train loss: 12.5082     \n",
      "INFO:Epoch 13 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5143     \n",
      "INFO:Epoch 14 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5043     \n",
      "INFO:Epoch 15 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5066     \n",
      "INFO:Epoch 16 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5072     \n",
      "INFO:Epoch 17 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5057     \n",
      "INFO:Epoch 18 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5081     \n",
      "INFO:Epoch 19 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5027     \n",
      "INFO:Epoch 20 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.4915     \n",
      "INFO:Epoch 21 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5108     \n",
      "INFO:Epoch 22 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5021     \n",
      "INFO:Epoch 23 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5029     \n",
      "INFO:Epoch 24 out of 40\n",
      "100/100 [==============================] - 3s - train loss: 12.5071     \n",
      "INFO:Epoch 25 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5047     \n",
      "INFO:Epoch 26 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5037     \n",
      "INFO:Epoch 27 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5046     \n",
      "INFO:Epoch 28 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5049     \n",
      "INFO:Epoch 29 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5072     \n",
      "INFO:Epoch 30 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5042     \n",
      "INFO:Epoch 31 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5005     \n",
      "INFO:Epoch 32 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5046     \n",
      "INFO:Epoch 33 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5014     \n",
      "INFO:Epoch 34 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5043     \n",
      "INFO:Epoch 35 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5029     \n",
      "INFO:Epoch 36 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5061     \n",
      "INFO:Epoch 37 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.4995     \n",
      "INFO:Epoch 38 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5024     \n",
      "INFO:Epoch 39 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5036     \n",
      "INFO:Epoch 40 out of 40\n",
      "100/100 [==============================] - 2s - train loss: 12.5029     \n",
      "DEBUG:findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/Users/lhd0430/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000\n",
      "DEBUG:findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/Users/lhd0430/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000\n"
     ]
    }
   ],
   "source": [
    "!python q3_gru.py predict -c gru -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[(1,2),(3,4),(5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 5), (2, 4, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(zip((1,2),(3,4),(5,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
