{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture Notes\n",
    "\n",
    "- Most of the models in this chapter is for single predictor. However, high-dim models exist.\n",
    "\n",
    "1. Polynomial Regression\n",
    "- f(x)\n",
    "- apply LS or logistic regression with X,X^2,...X^d predictors\n",
    "- Estimate the pointwise variance of f(x): compute pointwise confidence interval by using the covariance matrix of the coefs.\n",
    "\n",
    "2. Step Functions\n",
    "- cutpoints, dummy variables\n",
    "- apply LS or logistic regression with dummy variables\n",
    "- coefs are the averages of Y for each subdomain \n",
    "\n",
    "3. Regression Spline\n",
    "- Degree-d regression spline: piecewise degree-d polynomial, whose d-1 derivative is continuous at each knot \n",
    "- Basis function\n",
    "- Cubic spline with K knots: use K+3 predictors (X,X^2,X^3 and K basis) with K+4 coef.(degree of freedom)\n",
    "- Natual spline: regression spline which is linear at the boundary\n",
    "- Use CV to determine the best K\n",
    "\n",
    "4. Smoothing Splines\n",
    "- Loss + penalty\n",
    "- tuning parameter lambda controls the smoothness\n",
    "- effective degrees of freedom (a measure of the flexibility of the smoothing spline)\n",
    "    - Note: we don't use degrees of freedom as the parameters are heavily constrained by lambda\n",
    "- There will be a knot at each training observation\n",
    "- the fit function is a natural cubic spline (a shrunken version)\n",
    "- Use CV to determine the best lambda\n",
    "\n",
    "5. Local Regression\n",
    "- fraction of training points: s\n",
    "- weight function at each point: K\n",
    "- fit a weighted LSR\n",
    "- theoretically this can be implemented in higher dimensions\n",
    "    - Note: training observations in the neighbors are very few for dim>3\n",
    "\n",
    "6. Generalized Additive Methods (GAM)\n",
    "- A way to extend MLR by replacing each linear component beta_i*Xi with a smooth nonlinear function\n",
    "- Same idea for extending logistic regression\n",
    "- Fitting method: backfitting (fitting partial residual)\n",
    "- Pros\n",
    "    - no need to transform variables\n",
    "    - non-linear fits potentially make more accurate predictions\n",
    "    - additive -> can examine the effect of X_i on Y individually\n",
    "    - smoothness of each function can be summarized via degrees of freedom\n",
    "- Cons\n",
    "    - model is restricted to be additive. Important variable interations can be missed\n",
    "        - can manually add interaction terms or interaction functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polynomial regress: use pyspark.ml.feature.PolynomialExpansion \n",
    "# Others are not available in pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
