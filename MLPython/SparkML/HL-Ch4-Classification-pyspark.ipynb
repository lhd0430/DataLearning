{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture notes\n",
    "\n",
    "- Sample size: n\n",
    "- Predictor dim: p\n",
    "- Class numbers: k\n",
    "\n",
    "\n",
    "1. Logistic regression\n",
    "- logistic function p(X)\n",
    "- log-odds (logit) is linear in X\n",
    "- likelihood function: finding coefficients that maximize it. Number of coeff depends on predictor dim\n",
    "- z-stats\n",
    "\n",
    "2. Multiple logistic regression\n",
    "\n",
    "3. Discriminant analysis\n",
    "- Bayes' theorem\n",
    "- posterior prob. p_k(x) depends on pi_k (prior prob., easy to compute from Y), and f_k(x) (desity function of X, hard to get from X, but can assume simple forms)\n",
    "- when p=1\n",
    "    - assume f_k(x,mu_k,sigma) are Gaussian with the same variance\n",
    "    - the max of p_k(x) in k <=> the max of discriminant functions in k that is linear in x, which still depends on mu_k, sigma and pi_k\n",
    "    - need estimate pi_k, mu_k and sigma from X and Y\n",
    "    - Set pairs of discriminant functions equal to each other to determine Bayes decision boundary\n",
    "    - Once have k for given x, we can compute p_k(x) for the probability\n",
    "- when p>1\n",
    "    - assume f_k(x,mu_k,Sigma) are Gaussian with the same covariance\n",
    "    - algorithm same as p=1 case\n",
    "- Forms of discriminant analysis\n",
    "    - Linear: f_k(x) are Gaussian having the same covariance\n",
    "    - Quatratic: f_k(x) are Gaussian having different covariance\n",
    "    - Naive Bayes: X are independent in each class (covariance matrix is diagonal); useful for large p; useful for mixed feature vectors\n",
    "\n",
    "4. Evaluate threshold value\n",
    "- Confusion matrix\n",
    "- True/False positive/negative\n",
    "- Two types of error: True/False postitive rates\n",
    "- The Total Error is a weighted average of the False Positive Rate and False Negative Rate. The weights are determined by the Prior Probabilities of Positive and Negative Responses.\n",
    "- Visual method: ROC+AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame, DataFrameReader\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# change the path on your machine\n",
    "findspark.init(\"/Users/lhd0430/Downloads/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creat spark session\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "# Load data as pyspark.sql.DataFrame\n",
    "data = spark.read.csv(\"../data/Smarket.csv\", header=True, inferSchema=True)\n",
    "data.cache()\n",
    "print(data.count())\n",
    "data = data.dropna()\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert lable to numeric type\n",
    "data = data.replace(['Up','Down'],['1','0'],'Direction')\n",
    "data = data.withColumn('Direction',data[\"Direction\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07332176166857471\n",
      "[-9.89192479477e-05,-0.0730437218623,-0.0423082784084,0.0110973714617,0.00935025229529,0.0103156159254,0.134698461518]\n",
      "0.5386597145318075\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZLhLUoMMd0lq0P8Dvlqq/UsLS9EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e4768d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Convert feature to vector type\n",
    "vecAssembler = VectorAssembler(inputCols=data.columns[:-2], outputCol=\"features\")\n",
    "df = vecAssembler.transform(data)\n",
    "\n",
    "# Fit model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Direction\")\n",
    "ml = lr.fit(df)\n",
    "print(ml.intercept)\n",
    "print(ml.coefficients)\n",
    "\n",
    "# Predict\n",
    "predict = ml.transform(df)\n",
    "\n",
    "# Evaluate\n",
    "evaAUR = BinaryClassificationEvaluator(labelCol=\"Direction\",metricName=\"areaUnderROC\")\n",
    "aur = evaAUR.evaluate(predict)\n",
    "print(aur)\n",
    "# confusion matrix is not available in ml at this moment\n",
    "\n",
    "# Plot\n",
    "pddf = predict.toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(0,len(pddf.Direction)),pddf.Direction,c='b')\n",
    "plt.scatter(np.arange(0,len(pddf.Direction)),pddf.prediction,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2470.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [2001.0,0.381,-0.192,-2.624,-1.055,5.01,1.1913].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:232)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:165)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:163)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:173)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:117)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:77)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [2001.0,0.381,-0.192,-2.624,-1.055,5.01,1.1913].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:232)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:165)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:163)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-d2e007effbcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Direction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lhd0430/anaconda/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Users/lhd0430/anaconda/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lhd0430/anaconda/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lhd0430/anaconda/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lhd0430/anaconda/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lhd0430/anaconda/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2470.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [2001.0,0.381,-0.192,-2.624,-1.055,5.01,1.1913].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:232)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:165)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:163)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:173)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:117)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:77)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [2001.0,0.381,-0.192,-2.624,-1.055,5.01,1.1913].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:232)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$4.apply(NaiveBayes.scala:140)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:165)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$7.apply(NaiveBayes.scala:163)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# LDA - Naive Bayes\n",
    "\n",
    "# Note: NB here requires data to be nonnegative!!!!\n",
    "\n",
    "# Convert feature to vector type\n",
    "vecAssembler = VectorAssembler(inputCols=data.columns[:-2], outputCol=\"features\")\n",
    "df = vecAssembler.transform(data)\n",
    "\n",
    "# Fit model\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"Direction\")\n",
    "ml = nb.fit(df)\n",
    "\n",
    "# Predict\n",
    "predict = ml.transform(df)\n",
    "\n",
    "# Evaluate\n",
    "evaAcc = MulticlassClassificationEvaluator(labelCol=\"Direction\",metricName=\"accuracy\")\n",
    "acc = evaAUR.evaluate(predict)\n",
    "print(acc)\n",
    "# confusion matrix is not available in ml at this moment\n",
    "\n",
    "# Plot\n",
    "pddf = predict.toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(0,len(pddf.Direction)),pddf.Direction,c='b')\n",
    "plt.scatter(np.arange(0,len(pddf.Direction)),pddf.prediction,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
