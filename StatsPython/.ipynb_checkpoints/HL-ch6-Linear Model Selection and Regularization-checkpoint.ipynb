{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture notes\n",
    "\n",
    "- Sample size: n\n",
    "- Predictor dim: p\n",
    "- Parameter #: d\n",
    "\n",
    "1. Feature selection\n",
    "- Best subset selection\n",
    "    - Evaluate for all k=0...p predictors cases\n",
    "    - Not practical for large p\n",
    "- Stepwise selection\n",
    "    - Forward: add predictor 1by1 to meet the threshold\n",
    "    - Backword: remove predictor 1by1 to meet the threshold; only valid for n>p\n",
    "    - Hybrid\n",
    "    - Not guaranteed to yield the best model\n",
    "- Choose the optimal model\n",
    "    - Cp, AIC, BIC: depend on n, d, RSS and the estimate of the variance of epsi\n",
    "    - Adjusted R^2: depends on RSS, TSS, n, and d\n",
    "    - Validation and CV (estimate the test error without estimating the variance of epsi)\n",
    "        - One-standard-error rule\n",
    "        - Note: must partition the whole sample first before selecting predictors\n",
    "\n",
    "2. Shrinkage methods\n",
    "- Ridge regression\n",
    "    - tuning parameter: lambda\n",
    "    - minimize (RSS + l2 shrinkage penalty)\n",
    "    - shrinkage penalty controls the Var(f)\n",
    "    - better standardizing the predictor first\n",
    "- Lasso\n",
    "    - Use l1 norm in shrinkage penalty\n",
    "    - Note: for sufficiently lage lambda, beta could be exact zero, which is like performing a predictor selection\n",
    "    - \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
