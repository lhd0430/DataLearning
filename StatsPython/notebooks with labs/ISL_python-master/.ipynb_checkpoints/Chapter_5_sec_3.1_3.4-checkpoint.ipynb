{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import math\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto = pd.read_csv('data/Auto.csv', header=0, na_values='?')\n",
    "Auto = Auto.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "Auto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python and R use different random number generator, so we may see slightly difference results in this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "train = np.random.choice(Auto.shape[0], 196, replace=False)\n",
    "select = np.in1d(range(Auto.shape[0]), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.620\n",
      "Model:                            OLS   Adj. R-squared:                  0.618\n",
      "Method:                 Least Squares   F-statistic:                     316.4\n",
      "Date:                Fri, 09 Jun 2017   Prob (F-statistic):           1.28e-42\n",
      "Time:                        13:31:18   Log-Likelihood:                -592.07\n",
      "No. Observations:                 196   AIC:                             1188.\n",
      "Df Residuals:                     194   BIC:                             1195.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     40.3338      1.023     39.416      0.000      38.316      42.352\n",
      "horsepower    -0.1596      0.009    -17.788      0.000      -0.177      -0.142\n",
      "==============================================================================\n",
      "Omnibus:                        8.393   Durbin-Watson:                   1.061\n",
      "Prob(Omnibus):                  0.015   Jarque-Bera (JB):                8.787\n",
      "Skew:                           0.516   Prob(JB):                       0.0124\n",
      "Kurtosis:                       2.899   Cond. No.                         328.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "--------Test Error for 1st order--------\n",
      "23.3619028926\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "lm = smf.ols ('mpg~horsepower', data = Auto[select]).fit()\n",
    "print lm.summary()\n",
    "preds = lm.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print '--------Test Error for 1st order--------'\n",
    "print np.mean(square_error[~select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test Error for 2nd order--------\n",
      "20.2526908584\n"
     ]
    }
   ],
   "source": [
    "lm2 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0)', data = Auto[select]).fit()\n",
    "preds = lm2.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print '--------Test Error for 2nd order--------'\n",
    "print square_error[~select].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Test Error for 3rd order--------\n",
      "20.3256093659\n"
     ]
    }
   ],
   "source": [
    "lm3 = smf.ols ('mpg~horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0)', data = Auto[select]).fit()\n",
    "preds = lm3.predict(Auto)\n",
    "square_error = (Auto['mpg'] - preds)**2\n",
    "print '--------Test Error for 3rd order--------'\n",
    "print np.mean(square_error[~select])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### These results are consistent with our previous findings: a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is little evidence in favor of a model that uses a cubic function of horsepower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we look at the summmary for 3rd order regression, the coefficient of the 3rd order term is not statistically significant. I will use this as Supporting evidence for the above claim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.722\n",
      "Model:                            OLS   Adj. R-squared:                  0.717\n",
      "Method:                 Least Squares   F-statistic:                     165.9\n",
      "Date:                Fri, 09 Jun 2017   Prob (F-statistic):           4.60e-53\n",
      "Time:                        13:31:18   Log-Likelihood:                -561.56\n",
      "No. Observations:                 196   AIC:                             1131.\n",
      "Df Residuals:                     192   BIC:                             1144.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               66.5200      6.310     10.541      0.000      54.073      78.967\n",
      "horsepower              -0.6868      0.162     -4.238      0.000      -1.006      -0.367\n",
      "I(horsepower ** 2.0)     0.0028      0.001      2.157      0.032       0.000       0.005\n",
      "I(horsepower ** 3.0) -3.524e-06   3.27e-06     -1.078      0.282   -9.97e-06    2.92e-06\n",
      "==============================================================================\n",
      "Omnibus:                        9.054   Durbin-Watson:                   1.328\n",
      "Prob(Omnibus):                  0.011   Jarque-Bera (JB):               15.936\n",
      "Skew:                           0.174   Prob(JB):                     0.000346\n",
      "Kurtosis:                       4.353   Cond. No.                     5.83e+07\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.83e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print lm3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ols_fit = smf.ols ('mpg~horsepower', data = Auto).fit()\n",
    "print ols_fit.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM Fit. Compare with OLS fit, the coeffs are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept     39.935861\n",
      "horsepower    -0.157845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "glm_fit = sm.GLM.from_formula('mpg~horsepower', data = Auto).fit()\n",
    "print glm_fit.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying CV in Python is not as easy as that in R. It will require some manual coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use some of implemented function in Python, we use Sklearn for linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.9358610212\n",
      "[-0.15784473]\n"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(Auto.horsepower)\n",
    "y = Auto.mpg\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "print model.intercept_\n",
    "print model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.2315135179\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=x.shape[0]) # loo use folds equal to # of observations\n",
    "test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "print np.mean(-test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For higher order polynomial fit, we use pipline tool. Below shows how to fit an order 1 to 5 polynomial data and show the loo results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.231513517929226, 19.248213124489389, 19.334984064114092, 19.424430308545745, 19.033219754727607]\n"
     ]
    }
   ],
   "source": [
    "A = []\n",
    "for porder in xrange(1, 6):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits=x.shape[0]) # loo use folds equal to # of observations\n",
    "    test = cross_val_score(model, x, y, cv=k_fold,  scoring = 'neg_mean_squared_error', n_jobs=-1)\n",
    "    A.append(np.mean(-test))\n",
    "    \n",
    "print A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold validation is exactly same as LOO with different n_splits parameter setup. The computation time is much shorter than that of LOOCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.439933652339857, 21.235840055802118, 21.336606183328417, 21.35388698756352, 20.905633737044905, 20.78270442749729, 20.953103378424785, 21.077131628861984, 21.036781313639977, 20.980956456366364]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "A = []\n",
    "for porder in xrange(1, 11):\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=porder)), ('linear', LinearRegression())])\n",
    "    k_fold = KFold(n_splits=10) \n",
    "    test = cross_val_score(model, x, y, cv = k_fold,  scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "    A.append(np.mean(-test))\n",
    "    \n",
    "print A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap means sampling with replacement. To eliminate the effect of sample size, the norm practice is to sample the same size as original dataset with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Portfolio = pd.read_csv('data/Portfolio.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To illustrate the use of the bootstrap on this data, we must first create a function, alpha_fn(), which takes as input the (X, Y) data as well as a vector indicating which observations should be used to estimate alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alpha_fn(data, index):\n",
    "    X = data.X[index]\n",
    "    Y = data.Y[index]\n",
    "    return (np.var(Y) - np.cov(X,Y)[0,1])/(np.var(X) + np.var(Y) - 2 * np.cov(X, Y)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57665115161041181"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, range(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate one set of random index with 100 elements. The array has been sorted to show there are repeat elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  4,  7,  8,  8,  8,  9, 10, 15, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 22, 22, 26, 31, 31, 32, 33, 34, 34, 37, 38, 39, 39, 40, 40, 40,\n",
       "       42, 43, 43, 43, 43, 46, 46, 47, 49, 49, 50, 50, 51, 52, 52, 55, 56,\n",
       "       57, 58, 60, 61, 62, 63, 63, 66, 67, 67, 68, 68, 69, 70, 70, 70, 70,\n",
       "       72, 72, 73, 74, 75, 75, 76, 76, 79, 80, 81, 82, 82, 83, 83, 84, 85,\n",
       "       86, 87, 88, 90, 90, 90, 90, 90, 91, 95, 95, 96, 96, 97, 99])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall the previous function with a random set of input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.632327580798003"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_fn(Portfolio, np.random.choice(range(0, 100), size=100, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Since I am not aware of boot similar function in python, I just define a ad hoc function called boot_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boot_python(data, input_fun, iteration):\n",
    "    n = Portfolio.shape[0]\n",
    "    idx = np.random.randint(0, n, (iteration, n))\n",
    "    stat = np.zeros(iteration)\n",
    "    for i in xrange(len(idx)):\n",
    "        stat[i] = input_fun(data, idx[i])\n",
    "    \n",
    "    return {'Mean': np.mean(stat), 'STD': np.std(stat)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean': 0.58119008838974451, 'STD': 0.09408713019844589}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_python(Portfolio, alpha_fn, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar idea (boostrap) can be used in a lot of other places, such as estimating the accuracy of a linear regression model coeffcients / Conduct non-parametric testing (permutation test) / Estimate some complicated probability "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
