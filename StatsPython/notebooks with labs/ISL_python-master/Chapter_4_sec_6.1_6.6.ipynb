{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiuping1/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import math\n",
    "from patsy import dmatrices\n",
    "\n",
    "\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Smarket = pd.read_csv('data/Smarket.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "0  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "1  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "2  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "3  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "4  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Lag1',\n",
       " 'Lag2',\n",
       " 'Lag3',\n",
       " 'Lag4',\n",
       " 'Lag5',\n",
       " 'Volume',\n",
       " 'Today',\n",
       " 'Direction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smarket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For panda data frame, there is a method corr to compute pairwise correlation between numerical variables\n",
    "\"\"\"\n",
    "Smarket.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111db5290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "As one would expect, the correlations between the lag variables and today’s returns are close to zero. \n",
    "\"\"\"\n",
    "plt.plot(Smarket.ix[:, 6])\n",
    "#plt.plot(Smarket[['Volume']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is some known complications that in Sklearn about applying parameter regularization. This can be aviod to set the tuning parameter 'C' to a large number. Here to be consistent with R output, I decieded to use Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Direction[Down]  Direction[Up]\n",
      "0                 0.0            1.0\n",
      "1                 0.0            1.0\n",
      "2                 1.0            0.0\n",
      "3                 0.0            1.0\n",
      "4                 0.0            1.0\n",
      "5                 0.0            1.0\n",
      "6                 1.0            0.0\n",
      "7                 0.0            1.0\n",
      "8                 0.0            1.0\n",
      "9                 0.0            1.0\n",
      "10                1.0            0.0\n",
      "11                1.0            0.0\n",
      "12                0.0            1.0\n",
      "13                0.0            1.0\n",
      "14                1.0            0.0\n",
      "15                0.0            1.0\n",
      "16                1.0            0.0\n",
      "17                0.0            1.0\n",
      "18                1.0            0.0\n",
      "19                1.0            0.0\n",
      "20                1.0            0.0\n",
      "21                1.0            0.0\n",
      "22                0.0            1.0\n",
      "23                1.0            0.0\n",
      "24                1.0            0.0\n",
      "25                0.0            1.0\n",
      "26                1.0            0.0\n",
      "27                1.0            0.0\n",
      "28                1.0            0.0\n",
      "29                1.0            0.0\n",
      "...               ...            ...\n",
      "1220              0.0            1.0\n",
      "1221              0.0            1.0\n",
      "1222              0.0            1.0\n",
      "1223              0.0            1.0\n",
      "1224              0.0            1.0\n",
      "1225              0.0            1.0\n",
      "1226              1.0            0.0\n",
      "1227              0.0            1.0\n",
      "1228              1.0            0.0\n",
      "1229              0.0            1.0\n",
      "1230              0.0            1.0\n",
      "1231              1.0            0.0\n",
      "1232              0.0            1.0\n",
      "1233              1.0            0.0\n",
      "1234              1.0            0.0\n",
      "1235              0.0            1.0\n",
      "1236              0.0            1.0\n",
      "1237              0.0            1.0\n",
      "1238              0.0            1.0\n",
      "1239              1.0            0.0\n",
      "1240              1.0            0.0\n",
      "1241              1.0            0.0\n",
      "1242              1.0            0.0\n",
      "1243              0.0            1.0\n",
      "1244              0.0            1.0\n",
      "1245              0.0            1.0\n",
      "1246              1.0            0.0\n",
      "1247              0.0            1.0\n",
      "1248              1.0            0.0\n",
      "1249              1.0            0.0\n",
      "\n",
      "[1250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket, return_type = 'dataframe')\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>Direction[Up]</td>  <th>  No. Observations:  </th>  <td>  1250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 09 Jun 2017</td> <th>  Pseudo R-squ.:     </th> <td>0.002074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:53:32</td>     <th>  Log-Likelihood:    </th> <td> -863.79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -865.59</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td>0.7319</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.1260</td> <td>    0.241</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.598</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>   -0.0731</td> <td>    0.050</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.171</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0423</td> <td>    0.050</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.140</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag3</th>      <td>    0.0111</td> <td>    0.050</td> <td>    0.222</td> <td> 0.824</td> <td>   -0.087</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag4</th>      <td>    0.0094</td> <td>    0.050</td> <td>    0.187</td> <td> 0.851</td> <td>   -0.089</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag5</th>      <td>    0.0103</td> <td>    0.050</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.087</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Volume</th>    <td>    0.1354</td> <td>    0.158</td> <td>    0.855</td> <td> 0.392</td> <td>   -0.175</td> <td>    0.446</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:          Direction[Up]   No. Observations:                 1250\n",
       "Model:                          Logit   Df Residuals:                     1243\n",
       "Method:                           MLE   Df Model:                            6\n",
       "Date:                Fri, 09 Jun 2017   Pseudo R-squ.:                0.002074\n",
       "Time:                        13:53:32   Log-Likelihood:                -863.79\n",
       "converged:                       True   LL-Null:                       -865.59\n",
       "                                        LLR p-value:                    0.7319\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.1260      0.241     -0.523      0.601      -0.598       0.346\n",
       "Lag1          -0.0731      0.050     -1.457      0.145      -0.171       0.025\n",
       "Lag2          -0.0423      0.050     -0.845      0.398      -0.140       0.056\n",
       "Lag3           0.0111      0.050      0.222      0.824      -0.087       0.109\n",
       "Lag4           0.0094      0.050      0.187      0.851      -0.089       0.107\n",
       "Lag5           0.0103      0.050      0.208      0.835      -0.087       0.107\n",
       "Volume         0.1354      0.158      0.855      0.392      -0.175       0.446\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since we are more interested in stock marketing up, we take the second column of y as our response\n",
    "\"\"\"\n",
    "logit = sm.Logit(y.ix[:,1], X)\n",
    "logit.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept   -0.126000\n",
       "Lag1        -0.073074\n",
       "Lag2        -0.042301\n",
       "Lag3         0.011085\n",
       "Lag4         0.009359\n",
       "Lag5         0.010313\n",
       "Volume       0.135441\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().params # To extract the parameters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.50708413,  0.48146788,  0.48113883,  0.51522236,  0.51078116,\n",
       "        0.50695646,  0.49265087,  0.50922916,  0.51761353,  0.48883778,\n",
       "        0.4965211 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit().predict()[0:11] # To extract the probability of the market going up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up (1) or Down (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(np.zeros(shape=(1250,1)), columns = ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691034\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "predict_label.ix[logit.fit().predict()>0.5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can evalue the TRAINING result by constructing a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145, 457],\n",
       "       [141, 507]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y.ix[:,1], predict_label.ix[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. In this case, logistic regression correctly predicted the movement of the market 52.2% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52159999999999995"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y.ix[:,1] == predict_label.ix[:,0]) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Smarket_2005 = Smarket.query('Year >= 2005')\n",
    "Smarket_train = Smarket.query('Year < 2005')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the training dataset to build the logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume', Smarket_2005, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          Direction[Up]   No. Observations:                  998\n",
      "Model:                          Logit   Df Residuals:                      991\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 09 Jun 2017   Pseudo R-squ.:                0.001562\n",
      "Time:                        13:33:46   Log-Likelihood:                -690.55\n",
      "converged:                       True   LL-Null:                       -691.63\n",
      "                                        LLR p-value:                    0.9044\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1912      0.334      0.573      0.567      -0.463       0.845\n",
      "Lag1          -0.0542      0.052     -1.046      0.295      -0.156       0.047\n",
      "Lag2          -0.0458      0.052     -0.884      0.377      -0.147       0.056\n",
      "Lag3           0.0072      0.052      0.139      0.889      -0.094       0.108\n",
      "Lag4           0.0064      0.052      0.125      0.901      -0.095       0.108\n",
      "Lag5          -0.0042      0.051     -0.083      0.934      -0.104       0.096\n",
      "Volume        -0.1163      0.240     -0.485      0.628      -0.586       0.353\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit = sm.Logit(y_train.ix[:,1], X_train)\n",
    "print logit.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[77, 34],\n",
       "       [97, 44]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "mark = (preds > threshold).reset_index(drop=True)\n",
    "predict_label.ix[mark] = 1\n",
    "confusion_matrix(y_test.ix[:,1], predict_label.ix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48015873015873017"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period. The results are rather disappointing: the test error rate is 1 - 48% = 52 %, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retrain of the model with Lag1 and Lag2 will be similar to previous steps (I will omit those). Another way to deal with logistics regression is to change the threshold value from 0.5 to others. There is an example below with threshold 0.45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.691936\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56746031746031744"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.45\n",
    "predict_label.ix[(preds > threshold).reset_index(drop=True)] = 1\n",
    "confusion_matrix(y_test.ix[:,1], predict_label.ix[:,0])\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) # to get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692085\n",
      "         Iterations 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44047619047619047"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, X_train = dmatrices('Direction~Lag1+Lag2', Smarket_train, return_type = 'dataframe')\n",
    "y_test, X_test = dmatrices('Direction~Lag1+Lag2', Smarket_2005, return_type = 'dataframe')\n",
    "logit = sm.Logit(y_train.ix[:,1], X_train)\n",
    "preds = logit.fit().predict(X_test)\n",
    "predict_label = pd.DataFrame(np.zeros(shape=(X_test.shape[0],1)), columns = ['label'])\n",
    "threshold = 0.5\n",
    "confusion_matrix(y_test.ix[:,1], predict_label.ix[:,0])\n",
    "np.mean(y_test.iloc[:,1].reset_index(drop=True)==predict_label.iloc[:,0].reset_index(drop=True)) # to get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use sklearn's implementation of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn_lda = LDA(n_components=2) #creating a LDA object\n",
    "lda = sklearn_lda.fit(X_train.ix[:,1:3], y_train.ix[:,1]) #learning the projection matrix\n",
    "X_lda = lda.transform(X_train.ix[:,1:3]) #using the model to project X \n",
    "X_labels = lda.predict(X_train.ix[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = lda.predict_proba(X_train.ix[:,1:3]) #the probability of each sample to belong to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_labels=lda.predict(X_test.ix[:,1:3])\n",
    "X_test_prob = lda.predict_proba(X_test.ix[:,1:3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55952380952380953"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.ix[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's change the threshod a bit to see whether we can improve the accuracy. The 2nd column of X_test_prob is the probability belongs to UP group. The default value is 0.5, let us first check that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55952380952380953"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5 \n",
    "np.mean(y_test.ix[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56349206349206349"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.48\n",
    "np.mean(y_test.ix[:,1]==(X_test_prob[:,1]>=threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a little bit of annoying that QDA and LDA have minor difference in their parameter set-up and function names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599206349206\n"
     ]
    }
   ],
   "source": [
    "sklearn_qda = QDA(priors=None,store_covariances=True) #creating a QDA object\n",
    "qda = sklearn_qda.fit(X_train.ix[:,1:3], y_train.ix[:,1]) #learning the projection matrix\n",
    "X_labels = qda.predict(X_train.ix[:,1:3]) #gives you the predicted label for each sample\n",
    "X_prob = qda.predict_proba(X_train.ix[:,1:3]) #the probability of each sample to belong to each class\n",
    "\n",
    "X_test_labels=qda.predict(X_test.ix[:,1:3])\n",
    "X_test_prob = qda.predict_proba(X_test.ix[:,1:3]) \n",
    "\n",
    "print np.mean(y_test.ix[:,1]==X_test_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, use dir() to explore all the information stored in lda and qda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " 'classes_',\n",
       " 'covariances_',\n",
       " 'decision_function',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'means_',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'priors_',\n",
       " 'reg_param',\n",
       " 'rotations_',\n",
       " 'scalings_',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'store_covariances',\n",
       " 'tol']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "[array([[ 1.50662277, -0.03924806],\n",
      "       [-0.03924806,  1.53559498]]), array([[ 1.51700576, -0.02787349],\n",
      "       [-0.02787349,  1.49026815]])]\n"
     ]
    }
   ],
   "source": [
    "print qda.means_\n",
    "print qda.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.5 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531746031746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_fit_X',\n",
       " '_fit_method',\n",
       " '_get_param_names',\n",
       " '_init_params',\n",
       " '_pairwise',\n",
       " '_tree',\n",
       " '_y',\n",
       " 'algorithm',\n",
       " 'classes_',\n",
       " 'effective_metric_',\n",
       " 'effective_metric_params_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'kneighbors',\n",
       " 'kneighbors_graph',\n",
       " 'leaf_size',\n",
       " 'metric',\n",
       " 'metric_params',\n",
       " 'n_jobs',\n",
       " 'n_neighbors',\n",
       " 'outputs_2d_',\n",
       " 'p',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'radius',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'weights']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNN(n_neighbors= 3) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train.ix[:,1:3], y_train.ix[:,1]) #learning the projection matrix\n",
    "X_test_labels=KNN_fit.predict(X_test.ix[:,1:3])\n",
    "X_test_prob = KNN_fit.predict_proba(X_test.ix[:,1:3]) \n",
    "\n",
    "print np.mean(y_test.ix[:,1]==X_test_labels) \n",
    "\n",
    "dir(neigh) # use dir command to check what KNN offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.6 An Application to Caravan Insurance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Caravan = pd.read_csv('data/Caravan.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5822, 86)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  MGODOV  \\\n",
       "0       33         1        3         2         8       0       5       1   \n",
       "1       37         1        2         2         8       1       4       1   \n",
       "2       37         1        2         2         8       0       4       2   \n",
       "3        9         1        3         3         3       2       3       2   \n",
       "4       40         1        4         2        10       1       4       1   \n",
       "\n",
       "   MGODGE  MRELGE    ...     APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  \\\n",
       "0       3       7    ...            0        0        0       1        0   \n",
       "1       4       6    ...            0        0        0       1        0   \n",
       "2       4       3    ...            0        0        0       1        0   \n",
       "3       4       5    ...            0        0        0       1        0   \n",
       "4       4       7    ...            0        0        0       1        0   \n",
       "\n",
       "   APLEZIER  AFIETS  AINBOED  ABYSTAND  Purchase  \n",
       "0         0       0        0         0        No  \n",
       "1         0       0        0         0        No  \n",
       "2         0       0        0         0        No  \n",
       "3         0       0        0         0        No  \n",
       "4         0       0        0         0        No  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>ALEVEN</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "      <td>5822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.253349</td>\n",
       "      <td>1.110615</td>\n",
       "      <td>2.678805</td>\n",
       "      <td>2.991240</td>\n",
       "      <td>5.773617</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>4.626932</td>\n",
       "      <td>1.069907</td>\n",
       "      <td>3.258502</td>\n",
       "      <td>6.183442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076606</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.570079</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.031776</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.014256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.846706</td>\n",
       "      <td>0.405842</td>\n",
       "      <td>0.789835</td>\n",
       "      <td>0.814589</td>\n",
       "      <td>2.856760</td>\n",
       "      <td>1.003234</td>\n",
       "      <td>1.715843</td>\n",
       "      <td>1.017503</td>\n",
       "      <td>1.597647</td>\n",
       "      <td>1.909482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377569</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.080532</td>\n",
       "      <td>0.077403</td>\n",
       "      <td>0.562058</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.081632</td>\n",
       "      <td>0.210986</td>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.119996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MOSTYPE     MAANTHUI      MGEMOMV     MGEMLEEF     MOSHOOFD  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean     24.253349     1.110615     2.678805     2.991240     5.773617   \n",
       "std      12.846706     0.405842     0.789835     0.814589     2.856760   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%      10.000000     1.000000     2.000000     2.000000     3.000000   \n",
       "50%      30.000000     1.000000     3.000000     3.000000     7.000000   \n",
       "75%      35.000000     1.000000     3.000000     3.000000     8.000000   \n",
       "max      41.000000    10.000000     5.000000     6.000000    10.000000   \n",
       "\n",
       "            MGODRK       MGODPR       MGODOV       MGODGE       MRELGE  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      0.696496     4.626932     1.069907     3.258502     6.183442   \n",
       "std       1.003234     1.715843     1.017503     1.597647     1.909482   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     4.000000     0.000000     2.000000     5.000000   \n",
       "50%       0.000000     5.000000     1.000000     3.000000     6.000000   \n",
       "75%       1.000000     6.000000     2.000000     4.000000     7.000000   \n",
       "max       9.000000     9.000000     5.000000     9.000000     9.000000   \n",
       "\n",
       "          ...            ALEVEN     APERSONG      AGEZONG      AWAOREG  \\\n",
       "count     ...       5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      ...          0.076606     0.005325     0.006527     0.004638   \n",
       "std       ...          0.377569     0.072782     0.080532     0.077403   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "max       ...          8.000000     1.000000     1.000000     2.000000   \n",
       "\n",
       "            ABRAND      AZEILPL     APLEZIER       AFIETS      AINBOED  \\\n",
       "count  5822.000000  5822.000000  5822.000000  5822.000000  5822.000000   \n",
       "mean      0.570079     0.000515     0.006012     0.031776     0.007901   \n",
       "std       0.562058     0.022696     0.081632     0.210986     0.090463   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       7.000000     1.000000     2.000000     3.000000     2.000000   \n",
       "\n",
       "          ABYSTAND  \n",
       "count  5822.000000  \n",
       "mean      0.014256  \n",
       "std       0.119996  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       2.000000  \n",
       "\n",
       "[8 rows x 85 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caravan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale of the variables matters in KNN ! The core question in KNN is how to define proper distance metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of 1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of 1, 000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good way (?) to handle this problem is to standardize the data so that all standardize variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just scale() this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(np.zeros(shape=(Caravan.shape[0],1)), columns = ['label'])\n",
    "predict_label.ix[Caravan['Purchase'] == 'Yes'] = 1\n",
    "Caravan_drop = Caravan.drop(labels='Purchase', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I took a slightly different approach from the book. The training and testing data were splited by index. The normalization was done on the train set. Afterwards, the same normalization was applied to validate test.  The code might seem wordy, but it helps clear the logical flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "train_size = 1000\n",
    "train_index = xrange(0, train_size)\n",
    "X_validate = Caravan_drop.ix[train_index, ]\n",
    "Y_validate = predict_label.ix[train_index, ]\n",
    "X_train = Caravan_drop.ix[train_size:, ]\n",
    "Y_train = predict_label.ix[train_size:, ]\n",
    "\n",
    "\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with 1 neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883\n",
      "[[874  67]\n",
      " [ 50   9]]\n"
     ]
    }
   ],
   "source": [
    "neigh = KNN(n_neighbors= 1) # use n_neighbors to change the # of tune the performance of KNN\n",
    "KNN_fit = neigh.fit(X_train_scaled, Y_train.ix[:,0]) #learning the projection matrix\n",
    "X_validate_labels=KNN_fit.predict(X_validate_scaled)\n",
    "X_validate_prob = KNN_fit.predict_proba(X_validate_scaled) \n",
    "\n",
    "print np.mean(Y_validate.ix[:,0]==X_validate_labels) \n",
    "print confusion_matrix(Y_validate.ix[:,0], X_validate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rest of this exercise considers all the trade-off between False postive and False negative.  The concept of accuracy is NOT always the golden metric for classification problems. \n",
    "### Precision and recall, sensitivity and specificity, F1 score... are all reasonable metrics to consider. \n",
    "### We will discuss more on the concept of trainning, validation and test. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
